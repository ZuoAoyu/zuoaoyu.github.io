<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>WordPress on o5o</title><link>https://o5o.me/tags/wordpress/</link><description>Recent content in WordPress on o5o</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Mon, 13 Feb 2023 17:29:36 +0800</lastBuildDate><atom:link href="https://o5o.me/tags/wordpress/index.xml" rel="self" type="application/rss+xml"/><item><title>爬取WordPress网站：以爬虫的方式迁移WordPress至Hugo</title><link>https://o5o.me/post/crawler_requests_wordpress_site_convert_to_hugo_markdown/</link><pubDate>Mon, 13 Feb 2023 17:29:36 +0800</pubDate><guid>https://o5o.me/post/crawler_requests_wordpress_site_convert_to_hugo_markdown/</guid><description>写博客很多年了，以前用的都是WordPress，一个传统的PHP+MySQL的CMS。在2022年底开了一个新博客，用Hugo配合Obsidian来写，经过我的配置，在本地写好文章后只需git推送到github，github actions自动部署生成静态页面发送到服务器。很优雅的写作方式，所以从文章数量就能看出来，换用Hugo之后我是相当的高产。
所以我就想将这些年用WordPress写的文章都统一整理出来，集中用Hugo进行管理。很自然地我就想到了，使用python爬虫将所有文章和文章配图都爬取下来，将内容转换为Hugo能识别的Markdown格式。
分析 要爬取的网站：xiake.me（写本文时还是WordPress，未来绑定到Hugo博客） 使用的WP主题：Twenty Fifteen 文章列表URL：https://xiake.me/page/2/，其中2是指第2页。 文章URL：https://xiake.me/2020/08/06/一个傅里叶级数展开式的图象/，日期+文章标题或略缩名slug 文章配图URL：https://xiake.me/wp-content/uploads/2020/08/fuliye.png，日期+文件名 要获取的数据：
文章标题 文章slug（文章略缩名，用于固定链接） 文章URL 文章文字内容 文章配图 文章发布日期 思路：访问文章列表URL，获取文章标题、文章slug、文章URL，访问文章URL获取文章内容、文章发布日期，下载文章配图，并将图片链接替换为本地相对链接。
获取文章标题、URL这些数据使用XPATH，替换图片链接使用正则表达式。
用IPython踩坑 最终我们是要将程序用面向对象的方式来写，也就是把爬虫封装成一个类。但我们可以先用IPython把爬虫的运行过程走一遍，等各个环节都理顺了，把坑都存踩完了，再改造成想要的样子。
在这个环节，我先用Jupyter Lab用面向过程的思路把代码敲一遍。每写好一个代码块（cell）都可以按shift+enter（或ctrl+enter）运行一下，检查错误很方便。
先导入一些必定用到的模块，其他模块用到的时候再导入：
1 2 3 import requests from fake_useragent import UserAgent from lxml import etree 定义URL、请求头、获取响应对象：
1 2 3 4 5 6 url = &amp;#39;https://xiake.me&amp;#39; ua = UserAgent() headers = { &amp;#39;UserAgent&amp;#39;: ua.random, } res = requests.get(url, headers=headers) 将获取到的html内容打印出来看一看是不是成功获取到了：
1 2 html = res.content.decode(&amp;#39;utf-8&amp;#39;) print(html) 用XPATH解析页面，得到xpath对象：
1 2 3 4 5 # 解析html,得到文章元素 xpath_dbs = &amp;#39;/html/body/div/div[2]/div/main/article/header/h2/a&amp;#39; parse_html = etree.</description></item></channel></rss>